#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bç«™è§†é¢‘çˆ¬è™«ç¨‹åº - æ™ºèƒ½ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹Bç«™åçˆ¬è™«æœºåˆ¶çš„ä¼˜åŒ–ç‰ˆæœ¬

ä½œè€…ï¼šKirk
æ—¥æœŸï¼š2025-12-08
"""

import json
import os
import sys
import time
import random
import requests
from datetime import datetime
from typing import Dict, List, Optional
import warnings

# ç¦ç”¨SSLè­¦å‘Š
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except ImportError:
    pass


class BilibiliSmartCrawler:
    """Bç«™è§†é¢‘çˆ¬è™«ç±»ï¼ˆæ™ºèƒ½ç‰ˆæœ¬ï¼‰"""

    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«é…ç½®"""
        self.max_retries = 10  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.base_retry_delay = 15  # åŸºç¡€é‡è¯•å»¶è¿Ÿ
        self.base_request_delay = 8  # åŸºç¡€è¯·æ±‚é—´éš”
        self.videos_per_page = 5  # æ¯é¡µè§†é¢‘æ•°é‡ï¼ˆéå¸¸å°‘ï¼‰
        self.output_dir = "./output"
        self.consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
        self.last_success_time = None  # ä¸Šæ¬¡æˆåŠŸæ—¶é—´

        # å¤šä¸ªUser-Agentè½®æ¢
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)

    def get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        user_agent = random.choice(self.user_agents)
        return {
            'User-Agent': user_agent,
            'Referer': 'https://www.bilibili.com/',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': f'zh-CN,zh;q=0.{random.randint(8,9)},en;q=0.{random.randint(6,8)}',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': random.choice(['"macOS"', '"Windows"', '"Linux"']),
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
            'Origin': 'https://www.bilibili.com'
        }

    def smart_delay(self, base_delay, page=1):
        """æ™ºèƒ½å»¶è¿Ÿè®¡ç®—"""
        # åŸºç¡€å»¶è¿Ÿ + é¡µæ•°å½±å“ + éšæœºå› ç´  + è¿ç»­å¤±è´¥å½±å“
        page_factor = min(page * 2, 30)  # é¡µæ•°å½±å“ï¼Œæœ€å¤š30ç§’
        failure_factor = min(self.consecutive_failures * 10, 120)  # å¤±è´¥å½±å“ï¼Œæœ€å¤š120ç§’

        total_delay = base_delay + page_factor + failure_factor + random.uniform(5, 15)

        print(f"â±ï¸  æ™ºèƒ½å»¶è¿Ÿ {total_delay:.1f} ç§’ (é¡µæ•°:{page}, å¤±è´¥:{self.consecutive_failures})")
        time.sleep(total_delay)

    def make_request(self, url, params=None, description="è¯·æ±‚"):
        """å‘é€HTTPè¯·æ±‚"""
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    self.smart_delay(self.base_retry_delay)

                # æ¯æ¬¡éƒ½ä½¿ç”¨æ–°çš„è¯·æ±‚å¤´
                headers = self.get_random_headers()

                print(f"ğŸŒ æ­£åœ¨{description} (å°è¯• {attempt + 1}/{self.max_retries})")

                response = requests.get(
                    url,
                    params=params,
                    headers=headers,
                    timeout=30,
                    verify=False
                )
                response.raise_for_status()
                data = response.json()

                if data.get('code') == 0:
                    self.consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                    self.last_success_time = time.time()
                    return data.get('data', {})
                else:
                    error_msg = data.get('message', 'æœªçŸ¥é”™è¯¯')
                    if 'é¢‘ç¹' in error_msg or 'é¢‘ç‡' in error_msg or 'ä¸Šé™' in error_msg:
                        self.consecutive_failures += 1
                        wait_time = 60 + self.consecutive_failures * 30  # é€’å¢ç­‰å¾…æ—¶é—´
                        print(f"âš ï¸  è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âŒ {description}å¤±è´¥ï¼š{error_msg}")
                        self.consecutive_failures += 1
                        return None

            except requests.exceptions.Timeout:
                print(f"â° {description}è¶…æ—¶")
                self.consecutive_failures += 1
            except requests.exceptions.ConnectionError:
                print(f"ğŸ”Œ {description}è¿æ¥é”™è¯¯")
                self.consecutive_failures += 1
            except Exception as e:
                print(f"âŒ {description}å¼‚å¸¸ï¼š{e}")
                self.consecutive_failures += 1

        print(f"ğŸ’¥ {description}å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return None

    def get_user_info(self, uid: int) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨"""
        url = f"https://api.bilibili.com/x/space/arc/search"
        params = {
            'mid': uid,
            'ps': 1,
            'pn': 1
        }

        data = self.make_request(url, params, f"æ£€æŸ¥ç”¨æˆ· {uid}")
        return data is not None

    def init_save_file(self, uid: int) -> str:
        """åˆå§‹åŒ–ä¿å­˜æ–‡ä»¶"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        filename = f"videos_{uid}_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        init_data = {
            "user_info": {
                "uid": uid,
                "total_videos": 0,
                "start_time": current_time,
                "status": "crawling",
                "crawler_version": "smart_v1.0"
            },
            "videos": []
        }

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(init_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ åˆå§‹åŒ–ä¿å­˜æ–‡ä»¶ï¼š{filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æ–‡ä»¶å¤±è´¥ï¼š{e}")
            return ""

    def append_videos_to_file(self, filepath: str, new_videos: List[Dict]) -> bool:
        """å¢é‡æ·»åŠ è§†é¢‘åˆ°æ–‡ä»¶"""
        if not new_videos:
            return True

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            data['videos'].extend(new_videos)
            data['user_info']['total_videos'] = len(data['videos'])
            data['user_info']['last_update'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"âœ… å·²è¿½åŠ  {len(new_videos)} ä¸ªè§†é¢‘ï¼Œæ€»è®¡ {data['user_info']['total_videos']} ä¸ª")
            return True

        except Exception as e:
            print(f"âŒ è¿½åŠ è§†é¢‘å¤±è´¥ï¼š{e}")
            return False

    def finalize_save_file(self, filepath: str) -> str:
        """å®Œæˆæ–‡ä»¶ä¿å­˜"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            data['user_info']['status'] = 'completed'
            data['user_info']['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            dir_path = os.path.dirname(filepath)
            base_name = os.path.basename(filepath)
            final_name = base_name.replace('.json', '_final.json')
            final_path = os.path.join(dir_path, final_name)

            with open(final_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            os.remove(filepath)

            print(f"\nğŸ‰ æœ€ç»ˆæ–‡ä»¶å·²ä¿å­˜ï¼š{final_path}")
            print(f"ğŸ“Š æ€»è®¡çˆ¬å– {data['user_info']['total_videos']} ä¸ªè§†é¢‘")

            return final_path

        except Exception as e:
            print(f"âŒ å®Œæˆä¿å­˜å¤±è´¥ï¼š{e}")
            return filepath

    def fetch_all_videos(self, uid: int) -> List[Dict]:
        """è·å–ç”¨æˆ·çš„æ‰€æœ‰è§†é¢‘ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        all_videos = []
        page = 1

        print(f"\nğŸ¬ å¼€å§‹çˆ¬å–ç”¨æˆ· {uid} çš„è§†é¢‘åˆ—è¡¨")
        print("=" * 60)

        if not self.get_user_info(uid):
            print(f"âŒ ç”¨æˆ· {uid} ä¸å­˜åœ¨æˆ–æ²¡æœ‰å…¬å¼€è§†é¢‘")
            return []

        while True:
            if page > 1:
                self.smart_delay(self.base_request_delay, page)

            url = "https://api.bilibili.com/x/space/arc/search"
            params = {
                'mid': uid,
                'ps': self.videos_per_page,
                'pn': page,
                'order': 'pubdate'
            }

            data = self.make_request(url, params, f"è·å–ç¬¬ {page} é¡µ")

            if not data:
                print(f"ğŸ›‘ æ— æ³•è·å–ç¬¬ {page} é¡µï¼Œåœæ­¢çˆ¬å–")
                break

            videos = data.get('list', {}).get('vlist', [])

            if not videos:
                print(f"âœ… ç¬¬ {page} é¡µæ²¡æœ‰è§†é¢‘ï¼Œçˆ¬å–å®Œæˆ")
                break

            for i, video_info in enumerate(videos):
                video_data = {
                    'aid': video_info.get('aid'),
                    'bvid': video_info.get('bvid'),
                    'title': video_info.get('title'),
                    'url': f"https://www.bilibili.com/video/{video_info.get('bvid')}",
                    'duration': video_info.get('length'),
                    'created': video_info.get('created'),
                    'view': video_info.get('play'),
                    'danmaku': video_info.get('video_review'),
                    'reply': video_info.get('comment'),
                    'pic': video_info.get('pic'),
                    'description': video_info.get('description', '')
                }
                all_videos.append(video_data)

                if i < len(videos) - 1:
                    time.sleep(random.uniform(1, 3))

            print(f"âœ… ç¬¬ {page} é¡µå®Œæˆï¼š{len(videos)} ä¸ªè§†é¢‘ï¼Œæ€»è®¡ {len(all_videos)} ä¸ª")

            page_info = data.get('page', {})
            count = page_info.get('count', 0)
            if count > 0 and len(all_videos) >= count:
                print(f"âœ… å·²è·å–æ‰€æœ‰ {count} ä¸ªè§†é¢‘")
                break

            if len(videos) < self.videos_per_page:
                print(f"âœ… å½“å‰é¡µè§†é¢‘æ•°ä¸è¶³ï¼Œè¯´æ˜å·²åˆ°æœ€åä¸€é¡µ")
                break

            page += 1

            if page % 3 == 1:
                print(f"â˜• å·²çˆ¬å– {page-1} é¡µï¼Œå¼ºåˆ¶ä¼‘æ¯ 60 ç§’...")
                time.sleep(60)

        return all_videos

    def fetch_all_videos_with_incremental_save(self, uid: int) -> int:
        """è·å–ç”¨æˆ·çš„æ‰€æœ‰è§†é¢‘å¹¶å¢é‡ä¿å­˜"""
        save_filepath = self.init_save_file(uid)
        if not save_filepath:
            return 0

        total_videos = 0
        page = 1

        print(f"\nğŸ¬ å¼€å§‹çˆ¬å–ç”¨æˆ· {uid} çš„è§†é¢‘åˆ—è¡¨")
        print("=" * 60)
        print("ğŸ’¾ ä½¿ç”¨å¢é‡ä¿å­˜æ¨¡å¼ï¼Œæ•°æ®ä¼šå®æ—¶ä¿å­˜åˆ°æ–‡ä»¶")

        if not self.get_user_info(uid):
            print(f"âŒ ç”¨æˆ· {uid} ä¸å­˜åœ¨æˆ–æ²¡æœ‰å…¬å¼€è§†é¢‘")
            return 0

        print("\nğŸ“ æ³¨æ„ï¼šæ•°æ®ä¼šå®æ—¶ä¿å­˜ï¼Œå³ä½¿ç¨‹åºä¸­æ–­ä¹Ÿä¸ä¼šä¸¢å¤±å·²çˆ¬å–çš„æ•°æ®\n")

        while True:
            if page > 1:
                self.smart_delay(self.base_request_delay, page)

            url = "https://api.bilibili.com/x/space/arc/search"
            params = {
                'mid': uid,
                'ps': self.videos_per_page,
                'pn': page,
                'order': 'pubdate'
            }

            data = self.make_request(url, params, f"è·å–ç¬¬ {page} é¡µ")

            if not data:
                print(f"ğŸ›‘ æ— æ³•è·å–ç¬¬ {page} é¡µï¼Œåœæ­¢çˆ¬å–")
                break

            videos = data.get('list', {}).get('vlist', [])

            if not videos:
                print(f"âœ… ç¬¬ {page} é¡µæ²¡æœ‰è§†é¢‘ï¼Œçˆ¬å–å®Œæˆ")
                break

            page_videos = []
            for i, video_info in enumerate(videos):
                video_data = {
                    'aid': video_info.get('aid'),
                    'bvid': video_info.get('bvid'),
                    'title': video_info.get('title'),
                    'url': f"https://www.bilibili.com/video/{video_info.get('bvid')}",
                    'duration': video_info.get('length'),
                    'created': video_info.get('created'),
                    'view': video_info.get('play'),
                    'danmaku': video_info.get('video_review'),
                    'reply': video_info.get('comment'),
                    'pic': video_info.get('pic'),
                    'description': video_info.get('description', '')
                }
                page_videos.append(video_data)

                if i < len(videos) - 1:
                    time.sleep(random.uniform(1, 3))

            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
            if self.append_videos_to_file(save_filepath, page_videos):
                total_videos += len(page_videos)
                print(f"âœ… ç¬¬ {page} é¡µå®Œæˆï¼š{len(page_videos)} ä¸ªè§†é¢‘ï¼Œæ€»è®¡ {total_videos} ä¸ª")
            else:
                print(f"âŒ ç¬¬ {page} é¡µä¿å­˜å¤±è´¥")
                break

            page_info = data.get('page', {})
            count = page_info.get('count', 0)
            if count > 0 and total_videos >= count:
                print(f"âœ… å·²è·å–æ‰€æœ‰ {count} ä¸ªè§†é¢‘")
                break

            if len(videos) < self.videos_per_page:
                print(f"âœ… å½“å‰é¡µè§†é¢‘æ•°ä¸è¶³ï¼Œè¯´æ˜å·²åˆ°æœ€åä¸€é¡µ")
                break

            page += 1

            if page % 3 == 1:
                print(f"â˜• å·²çˆ¬å– {page-1} é¡µï¼Œå¼ºåˆ¶ä¼‘æ¯ 60 ç§’...")
                time.sleep(60)

        # å®Œæˆä¿å­˜
        self.finalize_save_file(save_filepath)

        return total_videos

    def save_to_json(self, uid: int, videos: List[Dict]) -> str:
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        data = {
            "user_info": {
                "uid": uid,
                "total_videos": len(videos),
                "crawl_time": current_time,
                "crawler_version": "smart_v1.0"
            },
            "videos": videos
        }

        filename = f"videos_{uid}_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°ï¼š{filepath}")
            return filepath

        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{e}")
            return ""

    def run(self, uid: Optional[int] = None) -> bool:
        """è¿è¡Œçˆ¬è™«ä¸»ç¨‹åº"""
        # è·å–UID
        if uid is None:
            try:
                uid_input = input("è¯·è¾“å…¥Bç«™ç”¨æˆ·UIDï¼š").strip()
                if not uid_input:
                    print("âŒ UIDä¸èƒ½ä¸ºç©º")
                    return False
                uid = int(uid_input)
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—UID")
                return False
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºå·²å–æ¶ˆ")
                return False

        print(f"\nğŸš€ å¯åŠ¨æ™ºèƒ½çˆ¬è™«ï¼Œç›®æ ‡ç”¨æˆ·ï¼š{uid}")
        print("âš ï¸  æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬åŒ…å«æ™ºèƒ½åçˆ¬è™«ç­–ç•¥ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        print("ğŸ’¾ ä½¿ç”¨å¢é‡ä¿å­˜æ¨¡å¼ï¼Œæ•°æ®ä¼šå®æ—¶ä¿å­˜åˆ°æ–‡ä»¶")

        # ä½¿ç”¨å¢é‡ä¿å­˜æ¨¡å¼çˆ¬å–è§†é¢‘
        total_videos = self.fetch_all_videos_with_incremental_save(uid)

        if total_videos == 0:
            print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘")
            return False

        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å–åˆ° {total_videos} ä¸ªè§†é¢‘")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼š")
        print(f"   æ€»è§†é¢‘æ•°ï¼š{total_videos}")
        print(f"   æ•°æ®å·²ä¿å­˜åˆ° output/ æ–‡ä»¶å¤¹")
        print(f"   æ–‡ä»¶åæ ¼å¼ï¼švideos_{uid}_æ—¶é—´æˆ³_final.json")

        print("\nğŸ’¡ æç¤ºï¼š")
        print(f"   - å³ä½¿ç¨‹åºä¸­é€”å¤±è´¥ï¼Œå·²çˆ¬å–çš„æ•°æ®ä¹Ÿå·²ä¿å­˜")
        print(f"   - å¯ä»¥æ£€æŸ¥ output/ æ–‡ä»¶å¤¹ä¸­çš„ä¸´æ—¶æ–‡ä»¶")
        print(f"   - æ™ºèƒ½ç‰ˆæœ¬æä¾›äº†æœ€ä½³çš„åçˆ¬è™«ç­–ç•¥")

        return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– Bç«™è§†é¢‘çˆ¬è™« - æ™ºèƒ½ç‰ˆæœ¬")
    print("=" * 50)
    print("ğŸ”§ ç‰¹æ€§ï¼šæ™ºèƒ½åçˆ¬è™«ã€è‡ªé€‚åº”å»¶è¿Ÿã€å¤šUser-Agentè½®æ¢")
    print("âš ï¸  æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä¸“ä¸ºç»•è¿‡åçˆ¬è™«æœºåˆ¶ä¼˜åŒ–ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†æˆåŠŸç‡æ›´é«˜")
    print()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    uid = None
    if len(sys.argv) > 1:
        try:
            uid = int(sys.argv[1])
        except ValueError:
            print("âŒ å‘½ä»¤è¡Œå‚æ•°å¿…é¡»æ˜¯æ•°å­—UID")
            return

    # è¿è¡Œçˆ¬è™«
    crawler = BilibiliSmartCrawler()
    success = crawler.run(uid)

    if success:
        print("\nğŸŠ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    else:
        print("\nğŸ’” ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()