# 💾 增量保存功能说明

## 功能特点

### 🔄 实时保存
- 每获取一页数据后立即保存到文件
- 不用等所有数据爬取完成才保存
- 避免长时间等待后失败导致数据丢失

### 📝 文件管理
- 爬取过程中：生成临时文件 `videos_UID_时间戳.json`
- 爬取完成后：自动重命名为 `videos_UID_时间戳_final.json`
- 程序中断时：临时文件保留，包含已爬取的数据

### 🛡️ 容错机制
- 程序中断不会丢失已爬取的数据
- 可以随时查看已保存的数据
- 支持断点续传（暂未实现）

## 使用方法

### 标准版本（增量保存）
```bash
python run.py
# 或
python bilibili_simple_crawler.py
```

### 智能版本（增量保存 + 反爬虫优化）
```bash
python run.py smart
# 或
python bilibili_smart_crawler.py
```

## 文件格式

### 爬取过程中的文件
```json
{
  "user_info": {
    "uid": 207321862,
    "total_videos": 25,
    "start_time": "2025-12-08 18:30:00",
    "status": "crawling",
    "last_update": "2025-12-08 18:35:00"
  },
  "videos": [
    {
      "aid": 123456789,
      "bvid": "BV1xxxxxx",
      "title": "视频标题",
      "url": "https://www.bilibili.com/video/BV1xxxxxx",
      ...
    }
  ]
}
```

### 完成后的文件
```json
{
  "user_info": {
    "uid": 207321862,
    "total_videos": 100,
    "start_time": "2025-12-08 18:30:00",
    "end_time": "2025-12-08 19:15:00",
    "status": "completed"
  },
  "videos": [
    // 所有视频数据
  ]
}
```

## 优势

### ✅ 数据安全
- **不怕中断**：即使程序崩溃，已保存的数据不会丢失
- **实时备份**：每一页数据都会立即保存
- **透明可见**：可以随时查看已爬取的进度

### ✅ 用户体验
- **减少等待**：不需要等到最后才知道是否有数据
- **进度可控**：可以随时查看爬取了多少数据
- **灵活中断**：需要时可以安全中断程序

### ✅ 资源优化
- **内存友好**：不需要在内存中保存所有数据
- **磁盘空间**：JSON格式，压缩效率高
- **可读性好**：JSON格式便于查看和处理

## 注意事项

1. **临时文件**：爬取过程中的临时文件也是有效的JSON格式
2. **文件覆盖**：同一UID多次运行会生成不同时间戳的文件
3. **磁盘空间**：大量视频会占用一定磁盘空间
4. **编码问题**：确保使用UTF-8编码，避免中文乱码

## 文件位置

所有保存的文件都在 `output/` 文件夹中：
- 临时文件：`videos_UID_时间戳.json`
- 最终文件：`videos_UID_时间戳_final.json`

## 示例输出

```
📝 初始化保存文件：./output/videos_207321862_20251208_183500.json
✅ 已追加 20 个视频，总计 20 个
✅ 第 1 页完成：20 个视频，总计 20 个
等待 8.0 秒后获取下一页...
✅ 已追加 20 个视频，总计 40 个
✅ 第 2 页完成：20 个视频，总计 40 个
...
🎉 最终文件已保存：./output/videos_207321862_20251208_183500_final.json
📊 总计爬取 100 个视频
```

现在你可以安心爬取大量视频了！即使程序中断，数据也会安全保存。